# ğŸ§  Contextual Emotion Detection

A deep learningâ€“based NLP application that detects human emotions from text using a **Bidirectional LSTM** model.  
The system captures contextual meaning, provides confidence-aware predictions, and exposes results through an interactive **Streamlit** web interface.

---

## ğŸš€ Key Features

- ğŸ” Classifies text into **6 emotion categories**
- ğŸ§  Uses **sequence-based deep learning (BiLSTM)** for contextual understanding
- ğŸ“Š Displays **confidence scores** and probability distribution
- âš ï¸ Flags **ambiguous or mixed-emotion inputs**
- ğŸŒ Deployed as an **interactive Streamlit application**
- âŒ No LLMs, no APIs â€” **pure deep learning**

---

## ğŸ­ Supported Emotion Classes

- ğŸ˜  Anger  
- ğŸ˜¨ Fear  
- ğŸ˜Š Joy  
- â¤ï¸ Love  
- ğŸ˜¢ Sadness  
- ğŸ˜² Surprise  

---

## ğŸ—ï¸ System Architecture

```mermaid
flowchart LR
    A[User Input Text] --> B[Text Preprocessing]
    B --> C[Tokenization]
    C --> D[Sequence Padding]
    D --> E[Bidirectional LSTM Model]
    E --> F[Softmax Probabilities]
    F --> G[Emotion Prediction]
    F --> H[Confidence Score]
    G --> I[Streamlit UI]
    H --> I
```

---

## ğŸ”„ Prediction Flow 
```mermaid
flowchart TD
    A[Input Sentence] --> B[Lowercasing & Cleaning]
    B --> C[Tokenizer.texts_to_sequences]
    C --> D[Pad Sequences to Max Length]
    D --> E[Embedding Layer]
    E --> F[Bidirectional LSTM]
    F --> G[Dense + Softmax]
    G --> H[Argmax â†’ Emotion Label]
    G --> I[Max Probability â†’ Confidence]
```

---

## ğŸ§  Model Details
- Architecture: Bidirectional LSTM

- Input Length: 100 tokens

- Embedding: Trainable embeddings

- Output: Softmax over 6 emotion classes

- Loss Function: Sparse Categorical Crossentropy

- Metric: Accuracy

- Performance: ~85% validation accuracy on unseen text

---

## ğŸ› ï¸ Tech Stack
| Category            | Tools / Technologies                                   |
|---------------------|--------------------------------------------------------|
| Language            | Python                                                 |
| Deep Learning       | TensorFlow, Keras                                      |
| NLP                 | Tokenization, Sequence Padding, LSTM-based Modeling    |
| Web Framework       | Streamlit                                              |
| Data Handling       | NumPy, Pandas                                          |
| Model Storage       | `.keras`, Pickle                                       |
| Version Control     | Git, GitHub                                            |

---

## ğŸ“ Project Structure
```
Contextual-Emotion-Detection/
â”‚
â”œâ”€â”€ app.py                     # Streamlit application
â”œâ”€â”€ emotion_lstm_model.keras   # Trained BiLSTM model
â”œâ”€â”€ tokenizer.pkl              # Fitted tokenizer
â”œâ”€â”€ label_encoder.pkl          # Emotion label encoder
â”œâ”€â”€ requirements.txt           # Dependencies
â”œâ”€â”€ README.md                  # Project documentation
```

---

## â–¶ï¸ How to Run Locally
### 1ï¸âƒ£ Clone the repository
```
git clone https://github.com/pranayk15/Contextual-Emotion-Detection.git
cd Contextual-Emotion-Detection
```

### 2ï¸âƒ£ Install dependencies
```
pip install -r requirements.txt
```

### 3ï¸âƒ£ Run the application
```
streamlit run app.py
```

---

## ğŸ“Š Output Example

- Predicted Emotion: JOY ğŸ˜Š

- Confidence: 0.82

- Probability Distribution: Visualized across all emotions

- Ambiguity Warning: Triggered when confidence < 0.5

---

## âš ï¸ Limitations

- Emotion detection is context-dependent, not keyword-based

- Sarcasm and irony may reduce confidence

- Model trained on general emotion datasets â€” domain-specific text may vary

---

## ğŸ‘¤ Author

**Pranay Kale** 
